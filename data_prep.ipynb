{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('filtered_data.csv')\n",
    "df_60, df_40 = train_test_split(df,test_size=0.40, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20_ac, df_20 = train_test_split(df_40,test_size=0.50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_60.to_csv('federated_data.csv', index=False)\n",
    "df_20_ac.to_csv('active_learning_data.csv', index=False)\n",
    "df_20.to_csv('test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save test numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wfdb\n",
    "import ast\n",
    "import tensorflow as tf\n",
    "\n",
    "def load_raw_data(df, sampling_rate, path):\n",
    "    if sampling_rate == 100:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename_lr]\n",
    "    else:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename_hr]\n",
    "    \n",
    "    data = np.array([signal for signal, meta in data])\n",
    "    return data\n",
    "path='ECG_data/'\n",
    "path_csv = 'test_data.csv'\n",
    "sampling_rate=100\n",
    "\n",
    "# load and convert annotation data\n",
    "Y = pd.read_csv(path_csv)\n",
    "\n",
    "# Load raw signal data\n",
    "X = load_raw_data(Y, sampling_rate, path)\n",
    "Y = Y['diagnostic_superclass'].values\n",
    "# Y = tf.keras.utils.to_categorical(Y,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4189, 1000, 12), (4189,))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_test.npy', X)\n",
    "np.save('Y_test.npy', Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 2364, 1: 1825})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "Counter(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active learning part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import wfdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "def load_raw_data(df, sampling_rate, path):\n",
    "    if sampling_rate == 100:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename_lr]\n",
    "    else:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename_hr]\n",
    "    \n",
    "    data = np.array([signal for signal, meta in data])\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_data():\n",
    "\n",
    "    path='ECG_data/'\n",
    "    path_csv = 'active_learning_data.csv'\n",
    "    sampling_rate=100\n",
    "\n",
    "    # load and convert annotation data\n",
    "    Y = pd.read_csv(path_csv)\n",
    "    # Load raw signal data\n",
    "    X = load_raw_data(Y, sampling_rate, path)\n",
    "    Y = Y['diagnostic_superclass'].values\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4189, 1000, 12)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Conv1D(filters=32, kernel_size=5, activation='relu', input_shape=(1000, 12)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    BatchNormalization(),\n",
    "    Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    BatchNormalization(),\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    BatchNormalization(),\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # For binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "262/262 [==============================] - 2s 6ms/step - loss: 0.5698 - accuracy: 0.8040\n",
      "Epoch 2/20\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 0.3744 - accuracy: 0.8341\n",
      "Epoch 3/20\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 0.3225 - accuracy: 0.8644\n",
      "Epoch 4/20\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 0.2817 - accuracy: 0.8728\n",
      "Epoch 5/20\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 0.2545 - accuracy: 0.8978\n",
      "Epoch 6/20\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 0.2653 - accuracy: 0.8945\n",
      "Epoch 7/20\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 0.2119 - accuracy: 0.9131\n",
      "Epoch 8/20\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 0.1991 - accuracy: 0.9231\n",
      "Epoch 9/20\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 0.1923 - accuracy: 0.9262\n",
      "Epoch 10/20\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 0.1964 - accuracy: 0.9231\n",
      "Epoch 11/20\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 0.1772 - accuracy: 0.9353\n",
      "Epoch 12/20\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 0.1453 - accuracy: 0.9429\n",
      "Epoch 13/20\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 0.1665 - accuracy: 0.9389\n",
      "Epoch 14/20\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 0.1420 - accuracy: 0.9453\n",
      "Epoch 15/20\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 0.1470 - accuracy: 0.9465\n",
      "Epoch 16/20\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 0.1268 - accuracy: 0.9554\n",
      "Epoch 17/20\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 0.1303 - accuracy: 0.9537\n",
      "Epoch 18/20\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 0.1300 - accuracy: 0.9556\n",
      "Epoch 19/20\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 0.0924 - accuracy: 0.9637\n",
      "Epoch 20/20\n",
      "262/262 [==============================] - 1s 4ms/step - loss: 0.1497 - accuracy: 0.9520\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, Y_train, epochs=20, batch_size=16, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.5544164776802063, accuracy: 0.8211984038352966\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x_test = np.load('X_test.npy')\n",
    "y_test = np.load('Y_test.npy')\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"loss = {loss}, accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('weights_active.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('federated_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnostic_superclass</th>\n",
       "      <th>filename_lr</th>\n",
       "      <th>filename_hr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>records100/01000/01553_lr</td>\n",
       "      <td>records500/01000/01553_hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>records100/06000/06835_lr</td>\n",
       "      <td>records500/06000/06835_hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>records100/14000/14561_lr</td>\n",
       "      <td>records500/14000/14561_hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>records100/07000/07371_lr</td>\n",
       "      <td>records500/07000/07371_hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>records100/05000/05478_lr</td>\n",
       "      <td>records500/05000/05478_hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12560</th>\n",
       "      <td>1</td>\n",
       "      <td>records100/11000/11739_lr</td>\n",
       "      <td>records500/11000/11739_hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12561</th>\n",
       "      <td>1</td>\n",
       "      <td>records100/12000/12466_lr</td>\n",
       "      <td>records500/12000/12466_hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12562</th>\n",
       "      <td>1</td>\n",
       "      <td>records100/05000/05609_lr</td>\n",
       "      <td>records500/05000/05609_hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12563</th>\n",
       "      <td>1</td>\n",
       "      <td>records100/00000/00899_lr</td>\n",
       "      <td>records500/00000/00899_hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12564</th>\n",
       "      <td>0</td>\n",
       "      <td>records100/16000/16477_lr</td>\n",
       "      <td>records500/16000/16477_hr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12565 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       diagnostic_superclass                filename_lr  \\\n",
       "0                          1  records100/01000/01553_lr   \n",
       "1                          0  records100/06000/06835_lr   \n",
       "2                          0  records100/14000/14561_lr   \n",
       "3                          1  records100/07000/07371_lr   \n",
       "4                          0  records100/05000/05478_lr   \n",
       "...                      ...                        ...   \n",
       "12560                      1  records100/11000/11739_lr   \n",
       "12561                      1  records100/12000/12466_lr   \n",
       "12562                      1  records100/05000/05609_lr   \n",
       "12563                      1  records100/00000/00899_lr   \n",
       "12564                      0  records100/16000/16477_lr   \n",
       "\n",
       "                     filename_hr  \n",
       "0      records500/01000/01553_hr  \n",
       "1      records500/06000/06835_hr  \n",
       "2      records500/14000/14561_hr  \n",
       "3      records500/07000/07371_hr  \n",
       "4      records500/05000/05478_hr  \n",
       "...                          ...  \n",
       "12560  records500/11000/11739_hr  \n",
       "12561  records500/12000/12466_hr  \n",
       "12562  records500/05000/05609_hr  \n",
       "12563  records500/00000/00899_hr  \n",
       "12564  records500/16000/16477_hr  \n",
       "\n",
       "[12565 rows x 3 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 7136, 1: 5429})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "Counter(df['diagnostic_superclass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
